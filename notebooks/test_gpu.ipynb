{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GPU requirements\n",
    "* Teest GPU requirements by downloading Qwen2.5-1.5B-Instruct model with unsloth\n",
    "* Create appropriate number of instances of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/s/Work/Projects/unsloth_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/s/Work/Projects/unsloth_env/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /mnt/s/hf\n",
      "TRANSFORMERS_CACHE: /mnt/s/hf/models\n",
      "Actual cache being used: /mnt/s/hf/models\n",
      "HF Hub cache: /mnt/s/hf/hub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import utils\n",
    "\n",
    "os.environ['HF_HOME'] = '/mnt/s/hf'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/s/hf/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/s/hf/datasets'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/mnt/s/hf/hub'\n",
    "\n",
    "utils.TRANSFORMERS_CACHE = '/mnt/s/hf/models'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('/mnt/s/hf/models', exist_ok=True)\n",
    "os.makedirs('/mnt/s/hf/datasets', exist_ok=True)\n",
    "os.makedirs('/mnt/s/hf/hub', exist_ok=True)\n",
    "\n",
    "# Verify the settings\n",
    "print(\"HF_HOME:\", os.getenv('HF_HOME'))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(\"Actual cache being used:\", utils.TRANSFORMERS_CACHE)\n",
    "print(\"HF Hub cache:\", os.getenv('HUGGINGFACE_HUB_CACHE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Output directory ready: ./resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen2.5-1.5B-Instruct\"  # Adjust this name if needed.\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True      # Set True to load in 4-bit (as often used with unsloth).\n",
    "OUTPUT_DIR = \"./resources\"  # Directory where model copies will be saved.\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"Output directory ready:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory (GB): 11.99\n",
      "Reserved GPU memory (GB): 0.00\n",
      "Allocated GPU memory (GB): 0.00\n",
      "Free GPU memory (GB): 11.99\n"
     ]
    }
   ],
   "source": [
    "device_id = 0  # or use torch.cuda.current_device()\n",
    "props = torch.cuda.get_device_properties(device_id)\n",
    "total_memory = props.total_memory  # in bytes\n",
    "reserved_memory = torch.cuda.memory_reserved(device_id)\n",
    "allocated_memory = torch.cuda.memory_allocated(device_id)\n",
    "free_memory = total_memory - reserved_memory\n",
    "\n",
    "print(\"Total GPU memory (GB): {:.2f}\".format(total_memory / 1024**3))\n",
    "print(\"Reserved GPU memory (GB): {:.2f}\".format(reserved_memory / 1024**3))\n",
    "print(\"Allocated GPU memory (GB): {:.2f}\".format(allocated_memory / 1024**3))\n",
    "print(\"Free GPU memory (GB): {:.2f}\".format(free_memory / 1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading QWEN2.5-1.5B via Unsloth...\n",
      "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model and tokenizer downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading QWEN2.5-1.5B via Unsloth...\")\n",
    "\n",
    "# Download the model using Unslothâ€™s FastLanguageModel API.\n",
    "# The API automatically handles special tokenizer and model configuration.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch.bfloat16,             # Use default dtype detection (or set \"bfloat16\" if supported)\n",
    "    load_in_4bit=LOAD_IN_4BIT\n",
    ")\n",
    "print(\"Model and tokenizer downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example: Patch the model with LoRA adapters for further tuning.\n",
    "# # This step is optional and can be modified as required.\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=16,  # LoRA rank parameter (choose according to your experimentation)\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0,\n",
    "#     bias=\"none\",\n",
    "#     use_gradient_checkpointing=\"unsloth\",  # Optimizes memory usage\n",
    "#     random_state=3407,\n",
    "#     use_rslora=False,\n",
    "#     loftq_config=None,\n",
    "# )\n",
    "# print(\"LoRA patching completed (if applied).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example: Patch the model with LoRA adapters for further tuning.\n",
    "# # This step is optional and can be modified as required.\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=16,  # LoRA rank parameter (choose according to your experimentation)\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0,\n",
    "#     bias=\"none\",\n",
    "#     use_gradient_checkpointing=\"unsloth\",  # Optimizes memory usage\n",
    "#     random_state=3407,\n",
    "#     use_rslora=False,\n",
    "#     loftq_config=None,\n",
    "# )\n",
    "# print(\"LoRA patching completed (if applied).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example: Patch the model with LoRA adapters for further tuning.\n",
    "# # This step is optional and can be modified as required.\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=16,  # LoRA rank parameter (choose according to your experimentation)\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0,\n",
    "#     bias=\"none\",\n",
    "#     use_gradient_checkpointing=\"unsloth\",  # Optimizes memory usage\n",
    "#     random_state=3407,\n",
    "#     use_rslora=False,\n",
    "#     loftq_config=None,\n",
    "# )\n",
    "# print(\"LoRA patching completed (if applied).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Measuring model instance 1:\n",
      "Memory added by the new model instance: 1.10 GB\n",
      "\n",
      "Measuring model instance 2:\n",
      "Memory added by the new model instance: 1.10 GB\n",
      "\n",
      "Measuring model instance 3:\n",
      "Memory added by the new model instance: 1.10 GB\n"
     ]
    }
   ],
   "source": [
    "def measure_model_gpu_usage(model, device=0):\n",
    "    # Clear cache to improve measurement consistency (optional)\n",
    "    torch.cuda.empty_cache()\n",
    "    # Synchronize to ensure that previous operations are complete.\n",
    "    torch.cuda.synchronize(device)\n",
    "    memory_before = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    # If you want to create an independent copy of the model,\n",
    "    # you can use deepcopy (be careful if the model has CUDA tensors already)\n",
    "    model_copy = copy.deepcopy(model).to('cuda')\n",
    "    torch.cuda.synchronize(device)\n",
    "    memory_after = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    usage_bytes = memory_after - memory_before\n",
    "    usage_gb = usage_bytes / (1024 ** 3)\n",
    "    print(f\"Memory added by the new model instance: {usage_gb:.2f} GB\")\n",
    "\n",
    "    return model_copy\n",
    "\n",
    "# Suppose `model` is already defined (for example, loaded from FastLanguageModel.from_pretrained)\n",
    "model_instances = []\n",
    "num_instances = 3\n",
    "\n",
    "for i in range(num_instances):\n",
    "    print(f\"\\nMeasuring model instance {i + 1}:\")\n",
    "    instance = measure_model_gpu_usage(model, device=0)\n",
    "    model_instances.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory (GB): 11.99\n",
      "Reserved GPU memory (GB): 4.40\n",
      "Allocated GPU memory (GB): 4.39\n",
      "Free GPU memory (GB): 7.59\n"
     ]
    }
   ],
   "source": [
    "# After creating all model instances, measure the current memory usage\n",
    "device_id = 0\n",
    "props = torch.cuda.get_device_properties(device_id)\n",
    "total_memory = props.total_memory\n",
    "reserved_memory = torch.cuda.memory_reserved(device_id)\n",
    "allocated_memory = torch.cuda.memory_allocated(device_id)\n",
    "free_memory = total_memory - reserved_memory\n",
    "\n",
    "print(\"Total GPU memory (GB): {:.2f}\".format(total_memory / 1024**3))\n",
    "print(\"Reserved GPU memory (GB): {:.2f}\".format(reserved_memory / 1024**3))\n",
    "print(\"Allocated GPU memory (GB): {:.2f}\".format(allocated_memory / 1024**3))\n",
    "print(\"Free GPU memory (GB): {:.2f}\".format(free_memory / 1024**3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
